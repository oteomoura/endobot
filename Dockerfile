# Use an official Node.js LTS image as the base
FROM node:18

# Set the working directory inside the container
WORKDIR /usr/src/app

<<<<<<< Updated upstream
# Install system dependencies, including python3-venv
RUN apt-get update && apt-get install -y python3-pip python3-venv wget
=======
# Install system dependencies required for Node.js
RUN apt-get update && apt-get install -y wget && rm -rf /var/lib/apt/lists/*
>>>>>>> Stashed changes

# Install Node.js dependencies first (optimizes caching)
COPY package.json package-lock.json ./
RUN npm install

# Define a build argument to enable or disable embedding insertion
ARG ENABLE_EMBEDDING=false

# Only run this logic if we need to insert new embeddings into the database
# and save memory resources on the server

<<<<<<< Updated upstream
# Install Python and ONNX dependencies inside a virtual environment
RUN python3 -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"
RUN pip install --no-cache-dir torch onnx onnxruntime transformers
=======
RUN if [ "$ENABLE_EMBEDDING" = "true" ]; then \
      echo "ðŸš€ Setting up embedding dependencies..."; \
      \
      # Install Python and required dependencies
      apt-get update && apt-get install -y python3-pip python3-venv && rm -rf /var/lib/apt/lists/* && \
      \
      # Set Hugging Face cache path
      export HF_HOME=/root/.cache/huggingface && \
      \
      # Create model directory and download necessary files
      mkdir -p $HF_HOME/hub/BAAI_bge-large-en-v1.5 && \
      cd $HF_HOME/hub/BAAI_bge-large-en-v1.5 && \
      wget -nc https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/config.json && \
      wget -nc https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/tokenizer.json && \
      wget -nc https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/tokenizer_config.json && \
      wget -nc https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/special_tokens_map.json && \
      wget -nc https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/vocab.txt && \
      wget -nc https://huggingface.co/BAAI/bge-large-en-v1.5/resolve/main/model.safetensors && \
      \
      # Install Python dependencies inside a virtual environment
      python3 -m venv /opt/venv && \
      export PATH="/opt/venv/bin:$PATH" && \
      pip install --no-cache-dir torch onnx onnxruntime transformers && \
      \
      # Run ONNX conversion
      python3 /usr/src/app/onnx_model_converter/script.py && \
      \
      # Validate that the ONNX model exists
      if [ ! -f $HF_HOME/hub/BAAI_bge-large-en-v1.5/onnx/model.onnx ]; then \
        echo "âŒ ONNX model was not created successfully" && exit 1; \
      else \
        echo "âœ… ONNX model was created successfully"; \
      fi \
    else \
      echo "âš ï¸ Skipping embedding setup (ENABLE_EMBEDDING=false)"; \
    fi
>>>>>>> Stashed changes

# Copy the application source code
COPY . .

# Expose the necessary port
EXPOSE 3000

# Start the application
CMD ["node", "server.js"]
